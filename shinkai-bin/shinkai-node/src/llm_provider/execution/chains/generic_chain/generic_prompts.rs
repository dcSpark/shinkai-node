use std::collections::HashMap;

use crate::llm_provider::execution::prompts::general_prompts::JobPromptGenerator;
use crate::llm_provider::providers::shared::openai_api::FunctionCallResponse;
use serde_json::json;
use shinkai_message_primitives::schemas::job::JobStepResult;
use shinkai_message_primitives::schemas::prompts::Prompt;
use shinkai_message_primitives::schemas::subprompts::SubPromptType;
use shinkai_tools_primitives::tools::shinkai_tool::ShinkaiTool;
use shinkai_vector_resources::vector_resource::RetrievedNode;

impl JobPromptGenerator {
    /// A basic generic prompt generator
    /// summary_text is the content generated by an LLM on parsing (if exist)
    #[allow(clippy::too_many_arguments)]
    pub fn generic_inference_prompt(
        custom_system_prompt: Option<String>,
        custom_user_prompt: Option<String>,
        user_message: String,
        image_files: HashMap<String, String>,
        ret_nodes: Vec<RetrievedNode>,
        _summary_text: Option<String>,
        job_step_history: Option<Vec<JobStepResult>>,
        tools: Vec<ShinkaiTool>,
        function_call: Option<FunctionCallResponse>,
    ) -> Prompt {
        let mut prompt = Prompt::new();

        // Add system prompt
        let system_prompt = custom_system_prompt
        .filter(|p| !p.trim().is_empty())
        .unwrap_or_else(|| "You are a very helpful assistant. You may be provided with documents or content to analyze and answer questions about them, in that case refer to the content provided in the user message for your responses.".to_string());

        prompt.add_content(system_prompt, SubPromptType::System, 98);

        let has_ret_nodes = !ret_nodes.is_empty();

        // Add previous messages
        // TODO: this should be full messages with assets and not just strings
        if let Some(step_history) = job_step_history {
            prompt.add_step_history(step_history, 97);
        }

        // Add tools if any. Decrease priority every 2 tools
        if !tools.is_empty() {
            let mut priority = 98;
            for (i, tool) in tools.iter().enumerate() {
                if let Ok(tool_content) = tool.json_function_call_format() {
                    prompt.add_tool(tool_content, SubPromptType::AvailableTool, priority);
                }
                if (i + 1) % 2 == 0 {
                    priority = priority.saturating_sub(1);
                }
            }
        }

        // Parses the retrieved nodes as individual sub-prompts, to support priority pruning
        // and also grouping i.e. instead of having 100 tiny messages, we have a message with the chunks grouped
        {
            if has_ret_nodes && !user_message.is_empty() {
                prompt.add_content("--- start --- \n".to_string(), SubPromptType::ExtraContext, 97);
            }
            eprintln!("Node content:\n");
            for node in ret_nodes {
                // Print the content of each node to the console
                eprintln!("{:?}", node.format_for_prompt(3500));
                prompt.add_ret_node_content(node, SubPromptType::ExtraContext, 96);
            }
            eprintln!("--- end ---");
            if has_ret_nodes && !user_message.is_empty() {
                prompt.add_content("--- end ---".to_string(), SubPromptType::ExtraContext, 97);
            }
        }

        // Add the user question and the preference prompt for the answer
        if !user_message.is_empty() {
            let user_prompt = custom_user_prompt.unwrap_or_default();
            let content = if user_prompt.is_empty() {
                user_message.clone()
            } else {
                format!("{}\n {}", user_message, user_prompt)
            };
            prompt.add_omni(content, image_files, SubPromptType::UserLastMessage, 100);
        }

        // If function_call exists, it means that the LLM requested a function call and we need to send the response back
        if let Some(function_call) = function_call {
            // Convert FunctionCall to Value
            let function_call_value = json!({
                "name": function_call.function_call.name,
                "arguments": function_call.function_call.arguments
            });

            // We add the assistant request to the prompt
            prompt.add_function_call(function_call_value, 100);

            // We add the function response to the prompt
            prompt.add_function_call_response(serde_json::to_value(function_call).unwrap(), 100);
        }

        prompt
    }
}
