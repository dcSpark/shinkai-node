use serde_json::json;
use shinkai_fs::shinkai_file_manager::ShinkaiFileManager;
use shinkai_sqlite::SqliteManager;
use std::{collections::HashMap, fs};

use crate::llm_provider::execution::prompts::general_prompts::JobPromptGenerator;
use crate::managers::tool_router::ToolCallFunctionResponse;

use crate::tools::tool_implementation::native_tools::sql_processor::get_current_tables;
use shinkai_message_primitives::schemas::prompts::Prompt;
use shinkai_message_primitives::schemas::shinkai_fs::ShinkaiFileChunkCollection;
use shinkai_message_primitives::schemas::subprompts::SubPromptType;
use shinkai_message_primitives::shinkai_message::shinkai_message::ShinkaiMessage;
use shinkai_tools_primitives::tools::shinkai_tool::ShinkaiTool;
use std::sync::Arc;

impl JobPromptGenerator {
    /// A basic generic prompt generator
    /// summary_text is the content generated by an LLM on parsing (if exist)
    #[allow(clippy::too_many_arguments)]
    pub async fn generic_inference_prompt(
        db: Arc<SqliteManager>,
        custom_system_prompt: Option<String>,
        custom_user_prompt: Option<String>,
        user_message: String,
        image_files: HashMap<String, String>,
        ret_nodes: ShinkaiFileChunkCollection,
        _summary_text: Option<String>,
        job_step_history: Option<Vec<ShinkaiMessage>>,
        tools: Vec<ShinkaiTool>,
        function_call: Option<ToolCallFunctionResponse>,
        job_id: String,
        additional_files: Vec<String>,
    ) -> Prompt {
        let mut prompt = Prompt::new();

        // Add system prompt
        let system_prompt = custom_system_prompt
        .filter(|p| !p.trim().is_empty())
        .unwrap_or_else(|| "You are a very helpful assistant. You may be provided with documents or content to analyze and answer questions about them, in that case refer to the content provided in the user message for your responses.".to_string());

        prompt.add_content(system_prompt, SubPromptType::System, 98);

        let has_ret_nodes = !ret_nodes.is_empty();

        // Add previous messages
        if let Some(step_history) = job_step_history {
            prompt.add_step_history(step_history, 97);
        }

        // Add tools if any. Decrease priority every 2 tools
        if !tools.is_empty() {
            let mut priority = 98;
            for (i, tool) in tools.iter().enumerate() {
                if let Ok(tool_content) = tool.json_function_call_format() {
                    match tool_content.get("function") {
                        Some(function) => {
                            let tool_router_key = function["tool_router_key"].as_str().unwrap_or("");
                            if tool_router_key == "local:::rust_toolkit:::shinkai_sqlite_query_executor" {
                                let job_id_clone = job_id.clone();
                                if let Ok(current_tables) = get_current_tables(job_id_clone).await {
                                    if !current_tables.is_empty() {
                                        prompt.add_content(
                                            format!(
                                                "<current_tables>\n{}\n</current_tables>\n",
                                                current_tables.join("; \n")
                                            ),
                                            SubPromptType::ExtraContext,
                                            97,
                                        );
                                    }
                                }
                            }
                        }
                        None => {}
                    }

                    prompt.add_tool(tool_content, SubPromptType::AvailableTool, priority);
                }
                if (i + 1) % 2 == 0 {
                    priority = priority.saturating_sub(1);
                }
            }
            let mut all_files = vec![];
            // Add job scope files
            let job_scope = ShinkaiFileManager::get_absolute_path_for_job_scope(&db, &job_id);
            if let Ok(job_scope) = job_scope {
                all_files.extend(job_scope);
            }
            // Add fs files and Agent files
            all_files.extend(additional_files);

            if !all_files.is_empty() {
                prompt.add_content(
                    format!("<current_files>\n{}\n</current_files>\n", all_files.join("\n")),
                    SubPromptType::ExtraContext,
                    97,
                );
            }
        }

        // Parses the retrieved nodes as individual sub-prompts, to support priority pruning
        // and also grouping i.e. instead of having 100 tiny messages, we have a message with the chunks grouped
        {
            if has_ret_nodes && !user_message.is_empty() {
                prompt.add_content("--- start --- \n".to_string(), SubPromptType::ExtraContext, 97);
            }

            prompt.add_ret_node_content(ret_nodes, SubPromptType::ExtraContext, 96);

            if has_ret_nodes && !user_message.is_empty() {
                prompt.add_content("--- end ---".to_string(), SubPromptType::ExtraContext, 97);
            }
        }

        // Add the user question and the preference prompt for the answer
        if !user_message.is_empty() {
            let mut content = user_message.clone();

            // If a custom user prompt is provided, use it as a template
            if let Some(template) = custom_user_prompt {
                let mut template_content = template.clone();

                // Insert user_message into the template
                if !template_content.contains("{{user_message}}") {
                    template_content.push_str(&format!("\n{}", user_message));
                } else {
                    template_content = template_content.replace("{{user_message}}", &user_message);
                }

                content = template_content;
            }

            prompt.add_omni(content, image_files, SubPromptType::UserLastMessage, 100);
        }

        // If function_call exists, it means that the LLM requested a function call and we need to send the response back
        if let Some(function_call) = function_call {
            // Convert FunctionCall to Value
            let function_call_value = json!({
                "name": function_call.function_call.name,
                "arguments": function_call.function_call.arguments
            });

            // We add the assistant request to the prompt
            prompt.add_function_call(function_call_value, 100);

            // We add the function response to the prompt
            prompt.add_function_call_response(serde_json::to_value(function_call).unwrap(), 100);
        }

        // eprintln!("prompt after adding function call: {:?}", prompt);
        prompt
    }
}

// TODO: add tests