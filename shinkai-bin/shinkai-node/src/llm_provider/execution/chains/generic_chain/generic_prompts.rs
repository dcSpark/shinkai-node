use super::super::super::prompts::prompts::{JobPromptGenerator, Prompt};
use crate::llm_provider::providers::shared::openai::FunctionCallResponse;
use crate::llm_provider::{execution::prompts::subprompts::SubPromptType, job::JobStepResult};
use crate::tools::shinkai_tool::ShinkaiTool;
use shinkai_vector_resources::vector_resource::RetrievedNode;

impl JobPromptGenerator {
    /// A basic generic prompt generator
    /// summary_text is the content generated by an LLM on parsing (if exist)
    #[allow(clippy::too_many_arguments)]
    pub fn generic_inference_prompt(
        custom_system_prompt: Option<String>,
        custom_user_prompt: Option<String>,
        user_message: String,
        ret_nodes: Vec<RetrievedNode>,
        summary_text: Option<String>,
        job_step_history: Option<Vec<JobStepResult>>,
        tools: Vec<ShinkaiTool>,
        function_call: Option<FunctionCallResponse>,
    ) -> Prompt {
        let mut prompt = Prompt::new();

        // Add system prompt
        let system_prompt = custom_system_prompt.unwrap_or_else(|| "You are a very helpful assistant.".to_string());
        prompt.add_content(system_prompt, SubPromptType::System, 98);

        // Add previous messages
        if let Some(step_history) = job_step_history {
            prompt.add_step_history(step_history, 97);
        }

        // If there is a document summary from the vector search add it with higher priority that the chunks
        if let Some(summary) = summary_text {
            prompt.add_content(
                format!(
                    "Here is the current summary of content another assistant found to answer the question: `{}`",
                    summary
                ),
                SubPromptType::User,
                99,
            );
        }

        let has_ret_nodes = !ret_nodes.is_empty();

        // Parses the retrieved nodes as individual sub-prompts, to support priority pruning
        // and also grouping i.e. instead of having 100 tiny messages, we have a message with the chunks grouped
        if has_ret_nodes {
            prompt.add_content(
                "Here is some extra context to answer any future user questions: --- start --- \n".to_string(),
                SubPromptType::ExtraContext,
                97,
            );
            for node in ret_nodes {
                prompt.add_ret_node_content(node, SubPromptType::ExtraContext, 96);
            }
            prompt.add_content("--- end ---".to_string(), SubPromptType::ExtraContext, 97);
        }

        // Add tools if any. Decrease priority every 2 tools
        if !tools.is_empty() {
            let mut priority = 98;
            for (i, tool) in tools.iter().enumerate() {
                if let Ok(tool_content) = tool.json_function_call_format() {
                    prompt.add_tool(tool_content, SubPromptType::AvailableTool, priority);
                }
                if (i + 1) % 2 == 0 {
                    priority = priority.saturating_sub(1);
                }
            }
        }

        // Add the user question and the preference prompt for the answer
        let user_prompt = custom_user_prompt.unwrap_or_else(|| {
            if has_ret_nodes {
                "Answer the question using the extra context provided.".to_string()
            } else {
                "Answer the question.".to_string()
            }
        });
        prompt.add_content(format!("{}\n {}", user_message, user_prompt), SubPromptType::User, 100);

        // If function_call exists, it means that the LLM requested a function call and we need to send the response back
        if let Some(function_call) = function_call {
            // We add the assistant request to the prompt
            prompt.add_function_call(function_call.function_call.clone(), 100);

            // We add the function response to the prompt
            prompt.add_function_call_response(function_call, 100);
        }

        prompt
    }
}
