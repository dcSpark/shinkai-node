use std::collections::HashMap;

use crate::llm_provider::execution::prompts::general_prompts::JobPromptGenerator;
use crate::managers::tool_router::ToolCallFunctionResponse;
use crate::network::v2_api::api_v2_commands_app_files::get_app_folder_path;
use crate::network::Node;
use crate::utils::environment::NodeEnvironment;
use serde_json::json;
use shinkai_message_primitives::schemas::job::JobStepResult;
use shinkai_message_primitives::schemas::prompts::Prompt;
use shinkai_message_primitives::schemas::subprompts::SubPromptType;
use shinkai_tools_primitives::tools::shinkai_tool::ShinkaiTool;
use shinkai_vector_resources::vector_resource::RetrievedNode;

impl JobPromptGenerator {
    /// A basic generic prompt generator
    /// summary_text is the content generated by an LLM on parsing (if exist)
    #[allow(clippy::too_many_arguments)]
    pub fn generic_inference_prompt(
        custom_system_prompt: Option<String>,
        custom_user_prompt: Option<String>,
        user_message: String,
        image_files: HashMap<String, String>,
        ret_nodes: Vec<RetrievedNode>,
        _summary_text: Option<String>,
        job_step_history: Option<Vec<JobStepResult>>,
        tools: Vec<ShinkaiTool>,
        function_call: Option<ToolCallFunctionResponse>,
        job_id: String,
        node_env: NodeEnvironment,
    ) -> Prompt {
        let mut prompt = Prompt::new();

        // Add system prompt
        let system_prompt = custom_system_prompt
        .filter(|p| !p.trim().is_empty())
        .unwrap_or_else(|| "You are a very helpful assistant. You may be provided with documents or content to analyze and answer questions about them, in that case refer to the content provided in the user message for your responses.".to_string());

        prompt.add_content(system_prompt, SubPromptType::System, 98);

        let has_ret_nodes = !ret_nodes.is_empty();

        // Add previous messages
        // TODO: this should be full messages with assets and not just strings
        if let Some(step_history) = job_step_history {
            prompt.add_step_history(step_history, 97);
        }

        let folder = get_app_folder_path(node_env, job_id);
        let current_files = Node::v2_api_list_app_files_internal(folder.clone(), true);
        if let Ok(current_files) = current_files {
            if !current_files.is_empty() {
                prompt.add_content(
                    format!("<current_files>\n{}\n</current_files>\n", current_files.join("\n")),
                    SubPromptType::ExtraContext,
                    97,
                );
            }
        }

        // Add tools if any. Decrease priority every 2 tools
        if !tools.is_empty() {
            let mut priority = 98;
            for (i, tool) in tools.iter().enumerate() {
                if let Ok(tool_content) = tool.json_function_call_format() {
                    prompt.add_tool(tool_content, SubPromptType::AvailableTool, priority);
                }
                if (i + 1) % 2 == 0 {
                    priority = priority.saturating_sub(1);
                }
            }
        }

        // Parses the retrieved nodes as individual sub-prompts, to support priority pruning
        // and also grouping i.e. instead of having 100 tiny messages, we have a message with the chunks grouped
        {
            if has_ret_nodes && !user_message.is_empty() {
                prompt.add_content("--- start --- \n".to_string(), SubPromptType::ExtraContext, 97);
            }
            for node in ret_nodes {
                prompt.add_ret_node_content(node, SubPromptType::ExtraContext, 96);
            }
            if has_ret_nodes && !user_message.is_empty() {
                prompt.add_content("--- end ---".to_string(), SubPromptType::ExtraContext, 97);
            }
        }

        // Add the user question and the preference prompt for the answer
        if !user_message.is_empty() {
            let mut content = user_message.clone();

            // If a custom user prompt is provided, use it as a template
            if let Some(template) = custom_user_prompt {
                let mut template_content = template.clone();

                // Insert user_message into the template
                if !template_content.contains("{{user_message}}") {
                    template_content.push_str(&format!("\n{}", user_message));
                } else {
                    template_content = template_content.replace("{{user_message}}", &user_message);
                }

                content = template_content;
            }

            prompt.add_omni(content, image_files, SubPromptType::UserLastMessage, 100);
        }

        // If function_call exists, it means that the LLM requested a function call and we need to send the response back
        if let Some(function_call) = function_call {
            // Convert FunctionCall to Value
            let function_call_value = json!({
                "name": function_call.function_call.name,
                "arguments": function_call.function_call.arguments
            });

            // We add the assistant request to the prompt
            prompt.add_function_call(function_call_value, 100);

            // We add the function response to the prompt
            prompt.add_function_call_response(serde_json::to_value(function_call).unwrap(), 100);
        }

        prompt
    }
}
