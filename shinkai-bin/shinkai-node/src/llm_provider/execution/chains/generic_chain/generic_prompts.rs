use serde_json::json;
use shinkai_sqlite::SqliteManager;
use std::{collections::HashMap, fs};

use crate::llm_provider::execution::prompts::general_prompts::JobPromptGenerator;
use crate::managers::tool_router::ToolCallFunctionResponse;

use crate::tools::tool_implementation::native_tools::sql_processor::get_current_tables;
use crate::utils::environment::NodeEnvironment;
use shinkai_message_primitives::schemas::shinkai_fs::ShinkaiFileChunkCollection;
use shinkai_message_primitives::schemas::subprompts::SubPromptType;
use shinkai_message_primitives::shinkai_message::shinkai_message::ShinkaiMessage;
use shinkai_message_primitives::{schemas::prompts::Prompt, shinkai_utils::job_scope::MinimalJobScope};
use shinkai_tools_primitives::tools::shinkai_tool::ShinkaiTool;
use std::sync::{mpsc, Arc};
use tokio::runtime::Runtime;

impl JobPromptGenerator {
    /// A basic generic prompt generator
    /// summary_text is the content generated by an LLM on parsing (if exist)
    #[allow(clippy::too_many_arguments)]
    pub fn generic_inference_prompt(
        custom_system_prompt: Option<String>,
        custom_user_prompt: Option<String>,
        user_message: String,
        image_files: HashMap<String, String>,
        ret_nodes: ShinkaiFileChunkCollection,
        _summary_text: Option<String>,
        job_step_history: Option<Vec<ShinkaiMessage>>,
        tools: Vec<ShinkaiTool>,
        function_call: Option<ToolCallFunctionResponse>,
        job_scope: MinimalJobScope,
        job_id: String,
        additional_files: Vec<String>,
        _node_env: NodeEnvironment,
        _db: Arc<SqliteManager>,
    ) -> Prompt {
        let mut prompt = Prompt::new();

        // Add system prompt
        let system_prompt = custom_system_prompt
        .filter(|p| !p.trim().is_empty())
        .unwrap_or_else(|| "You are a very helpful assistant. You may be provided with documents or content to analyze and answer questions about them, in that case refer to the content provided in the user message for your responses.".to_string());

        prompt.add_content(system_prompt, SubPromptType::System, 98);

        let has_ret_nodes = !ret_nodes.is_empty();

        // Add previous messages
        if let Some(step_history) = job_step_history {
            prompt.add_step_history(step_history, 97);
        }

        // Add tools if any. Decrease priority every 2 tools
        if !tools.is_empty() {
            let mut priority = 98;
            for (i, tool) in tools.iter().enumerate() {
                if let Ok(tool_content) = tool.json_function_call_format() {
                    match tool_content.get("function") {
                        Some(function) => {
                            let tool_router_key = function["tool_router_key"].as_str().unwrap_or("");
                            if tool_router_key == "local:::rust_toolkit:::shinkai_sqlite_query_executor" {
                                let (tx, rx) = mpsc::channel();
                                let job_id_clone = job_id.clone();
                                // Spawn the async task on a runtime
                                std::thread::spawn(move || {
                                    let runtime = Runtime::new().unwrap();
                                    let result = runtime.block_on(get_current_tables(job_id_clone));
                                    tx.send(result).unwrap();
                                });
                                // Wait for the result
                                let current_tables = rx.recv().unwrap();
                                if let Ok(current_tables) = current_tables {
                                    if !current_tables.is_empty() {
                                        prompt.add_content(
                                            format!(
                                                "<current_tables>\n{}\n</current_tables>\n",
                                                current_tables.join("; \n")
                                            ),
                                            SubPromptType::ExtraContext,
                                            97,
                                        );
                                    }
                                }
                            }
                        }
                        None => {}
                    }

                    prompt.add_tool(tool_content, SubPromptType::AvailableTool, priority);
                }
                if (i + 1) % 2 == 0 {
                    priority = priority.saturating_sub(1);
                }
            }

            // job_scope.
            //     {
            //         // Process fs_files_paths
            // for path in &fs_files_paths {
            //     if let Some(parsed_file) = sqlite_manager.get_parsed_file_by_shinkai_path(path).unwrap() {
            //         let file_id = parsed_file.id.unwrap();
            //         parsed_file_ids.push(file_id);
            //         paths_map.insert(file_id, path.clone());

            //         if let Some(file_tokens) = parsed_file.total_tokens {
            //             total_tokens += file_tokens;
            //         } else {
            //             all_files_have_token_count = false;
            //         }
            //     }
            // }

            // // Process job_filenames
            // for filename in &job_filenames {
            //     let file_path =
            //         match ShinkaiFileManager::construct_job_file_path(&job_id, filename, &sqlite_manager) {
            //             Ok(path) => path,
            //             Err(_) => continue,
            //         };

            //     if let Some(parsed_file) = sqlite_manager.get_parsed_file_by_shinkai_path(&file_path).unwrap() {
            //         let file_id = parsed_file.id.unwrap();
            //         parsed_file_ids.push(file_id);
            //         paths_map.insert(file_id, file_path);

            //         if let Some(file_tokens) = parsed_file.total_tokens {
            //             total_tokens += file_tokens;
            //         } else {
            //             all_files_have_token_count = false;
            //         }
            //     }
            // }

            // Retrieve each file in the job scope
            // for path in &job_scope.vector_fs_items {
            //     if let Some(parsed_file) = db.get_parsed_file_by_shinkai_path(path).unwrap() {
            //         let file_id = parsed_file.id.unwrap();
            //         parsed_file_ids.push(file_id);
            //         paths_map.insert(file_id, path.clone());

            //         // Count tokens while we're here
            //         if let Some(file_tokens) = parsed_file.total_tokens {
            //             total_tokens += file_tokens;
            //         } else {
            //             all_files_have_token_count = false;
            //         }
            //     }
            // }

            // // Process fs_folder_paths
            // for folder in &fs_folder_paths {
            //     Self::process_folder_contents(
            //         folder,
            //         &sqlite_manager,
            //         &mut parsed_file_ids,
            //         &mut paths_map,
            //         &mut total_tokens,
            //         &mut all_files_have_token_count,
            //     )
            //     .await?;
            // }

            // // Retrieve files inside vector_fs_folders
            // for folder in &scope.vector_fs_folders {
            //     Self::process_folder_contents(
            //         folder,
            //         &sqlite_manager,
            //         &mut parsed_file_ids,
            //         &mut paths_map,
            //         &mut total_tokens,
            //         &mut all_files_have_token_count,
            //     )
            //     .await?;
            // }

            // }
            // job_id -> full-job -> job.scope -> shinkai-pathws -> file-info
            let mut file_in_scope = additional_files.clone();
            // let folder_path = db.get_job_folder_name(&job_id);
            // if let Ok(folder_path) = folder_path {
            job_scope.vector_fs_items.iter().for_each(|path| {
                let file_path = format!("{:?}", fs::canonicalize(path.path.clone()).unwrap_or_default());
                file_in_scope.push(file_path.replace("\"", ""));
            });
            // }
            if !file_in_scope.is_empty() {
                prompt.add_content(
                    format!("<cursrent_files>\n{}\n</current_files>\n", file_in_scope.join("\n")),
                    SubPromptType::ExtraContext,
                    97,
                );
            }

            // let files_result = ShinkaiFileManager::get_all_files_and_folders_for_job(&job_id, &db);
            // if let Ok(folder_path) = folder_path {
            //     if let Ok(files_result) = files_result {
            //         let files_result_str = files_result
            //             .iter()
            //             .map(|file| {
            //                 format!(
            //                     "{:?}/{}",
            //                     fs::canonicalize(folder_path.path.clone()).unwrap_or_default(),
            //                     file.name
            //                 )
            //             })
            //             .collect::<Vec<String>>();
            //         prompt.add_content(
            //             format!("<current_files>\n{}\n</current_files>\n", files_result_str.join("\n")),
            //             SubPromptType::ExtraContext,
            //             97,
            //         );
            //     }
            // }
        }

        // Parses the retrieved nodes as individual sub-prompts, to support priority pruning
        // and also grouping i.e. instead of having 100 tiny messages, we have a message with the chunks grouped
        {
            if has_ret_nodes && !user_message.is_empty() {
                prompt.add_content("--- start --- \n".to_string(), SubPromptType::ExtraContext, 97);
            }

            prompt.add_ret_node_content(ret_nodes, SubPromptType::ExtraContext, 96);

            if has_ret_nodes && !user_message.is_empty() {
                prompt.add_content("--- end ---".to_string(), SubPromptType::ExtraContext, 97);
            }
        }

        // Add the user question and the preference prompt for the answer
        if !user_message.is_empty() {
            let mut content = user_message.clone();

            // If a custom user prompt is provided, use it as a template
            if let Some(template) = custom_user_prompt {
                let mut template_content = template.clone();

                // Insert user_message into the template
                if !template_content.contains("{{user_message}}") {
                    template_content.push_str(&format!("\n{}", user_message));
                } else {
                    template_content = template_content.replace("{{user_message}}", &user_message);
                }

                content = template_content;
            }

            prompt.add_omni(content, image_files, SubPromptType::UserLastMessage, 100);
        }

        // If function_call exists, it means that the LLM requested a function call and we need to send the response back
        if let Some(function_call) = function_call {
            // Convert FunctionCall to Value
            let function_call_value = json!({
                "name": function_call.function_call.name,
                "arguments": function_call.function_call.arguments
            });

            // We add the assistant request to the prompt
            prompt.add_function_call(function_call_value, 100);

            // We add the function response to the prompt
            prompt.add_function_call_response(serde_json::to_value(function_call).unwrap(), 100);
        }

        prompt
    }
}
